{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-10T19:53:45.612599Z",
     "iopub.status.busy": "2025-09-10T19:53:45.611900Z",
     "iopub.status.idle": "2025-09-10T19:53:49.166839Z",
     "shell.execute_reply": "2025-09-10T19:53:49.166008Z",
     "shell.execute_reply.started": "2025-09-10T19:53:45.612572Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers accelerate datasets peft bitsandbytes evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-09-10T19:53:49.168824Z",
     "iopub.status.busy": "2025-09-10T19:53:49.168527Z",
     "iopub.status.idle": "2025-09-10T19:55:57.850526Z",
     "shell.execute_reply": "2025-09-10T19:55:57.849383Z",
     "shell.execute_reply.started": "2025-09-10T19:53:49.168791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_bert_mnli_kaggle.py\n",
    "# Fine-tune BERT-base on MNLI and push final model to HF Hub repo \"{HF_USERNAME}/BERT-Base-MNLI-Orig\"\n",
    "import os\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoConfig\n",
    ")\n",
    "import evaluate\n",
    "from huggingface_hub import HfApi, login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "def push_folder_to_hub(folder_path, repo_id, token):\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id=repo_id, token=token, exist_ok=True)\n",
    "    api.upload_folder(folder_path=folder_path, repo_id=repo_id, token=token, repo_type=\"model\")\n",
    "\n",
    "def preprocess_batch(examples, tokenizer, max_length=256):\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, max_length=max_length)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_id\", default=\"bert-base-uncased\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"/kaggle/working/bert_mnli_out\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--train_bs\", type=int, default=8)\n",
    "    parser.add_argument(\"--eval_bs\", type=int, default=16)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--max_len\", type=int, default=256)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    # HF secrets (Kaggle)\n",
    "    user_secrets = UserSecretsClient()\n",
    "    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "    HF_USERNAME = user_secrets.get_secret(\"HF_USERNAME\")\n",
    "    if not HF_TOKEN or not HF_USERNAME:\n",
    "        raise RuntimeError(\"Set HF_TOKEN and HF_USERNAME as Kaggle Secrets before running.\")\n",
    "    repo_id = f\"{HF_USERNAME}/BERT-Base-MNLI-Orig\"\n",
    "\n",
    "    # Login for uploads\n",
    "    login(token=HF_TOKEN)\n",
    "\n",
    "    # Load tokenizer & (sequence-classification) model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, use_fast=True)\n",
    "    # Create config with correct num_labels\n",
    "    cfg = AutoConfig.from_pretrained(args.model_id)\n",
    "    cfg.num_labels = 3\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_id, config=cfg)\n",
    "\n",
    "    # Load MNLI\n",
    "    ds = load_dataset(\"multi_nli\")\n",
    "    train = ds[\"train\"]\n",
    "    val_matched = ds[\"validation_matched\"]\n",
    "    val_mismatched = ds[\"validation_mismatched\"]\n",
    "\n",
    "    # Preprocess (batched)\n",
    "    train = train.map(lambda x: preprocess_batch(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in train.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "    val_matched = val_matched.map(lambda x: preprocess_batch(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in val_matched.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "    val_mismatched = val_mismatched.map(lambda x: preprocess_batch(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in val_mismatched.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "\n",
    "    # Combine matched + mismatched as test set for final evaluation\n",
    "    from datasets import concatenate_datasets\n",
    "\n",
    "# Combine two datasets\n",
    "    test = concatenate_datasets([val_matched, val_mismatched])\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "    print(test[:2])\n",
    "    # Metrics\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    metric_prec = evaluate.load(\"precision\")\n",
    "    metric_rec = evaluate.load(\"recall\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "            \"precision_macro\": metric_prec.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "            \"recall_macro\": metric_rec.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "            \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        }\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_bs,\n",
    "        per_device_eval_batch_size=args.eval_bs,\n",
    "        learning_rate=args.lr,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=500,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        remove_unused_columns=True,\n",
    "        fp16=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=test,   # evaluate on combined validation during training\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Final evaluation on test (validation_matched + validation_mismatched)\n",
    "    eval_res = trainer.evaluate(eval_dataset=test)\n",
    "    print(\"Final evaluation (combined val_matched + val_mismatched):\")\n",
    "    print(eval_res)\n",
    "\n",
    "    # Save locally and push to Hugging Face hub\n",
    "    final_dir = os.path.join(args.output_dir, \"final_model\")\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    model.save_pretrained(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "    push_folder_to_hub(final_dir, repo_id, HF_TOKEN)\n",
    "    print(\"Pushed model to:\", repo_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debiased Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-09-10T19:56:04.995596Z",
     "iopub.status.busy": "2025-09-10T19:56:04.994778Z",
     "iopub.status.idle": "2025-09-10T19:56:05.001567Z",
     "shell.execute_reply": "2025-09-10T19:56:05.001010Z",
     "shell.execute_reply.started": "2025-09-10T19:56:04.995557Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File present: True data/debiased/mnli_aug-z.jsonl\n"
     ]
    }
   ],
   "source": [
    "#Create and process debiased MNLI dataset\n",
    "\n",
    "# Create folder and download debiased MNLI JSONL (works on Kaggle)\n",
    "import os, sys, subprocess\n",
    "os.makedirs('data/debiased', exist_ok=True)\n",
    "# Prefer wget on Kaggle; fallback to curl if needed\n",
    "url = 'https://storage.googleapis.com/allennlp-public-data/gen-debiased-nli/mnli_z-aug.jsonl'\n",
    "out = 'data/debiased/mnli_aug-z.jsonl'\n",
    "if not os.path.exists(out):\n",
    "    try:\n",
    "        # Try wget first\n",
    "        subprocess.run(['wget', '-O', out, url], check=True)\n",
    "    except Exception:\n",
    "        # Fallback to curl\n",
    "        subprocess.run(['curl', '-L', url, '-o', out], check=True)\n",
    "print('File present:', os.path.exists(out), out)\n",
    "\n",
    "import random, numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T19:56:07.749517Z",
     "iopub.status.busy": "2025-09-10T19:56:07.748702Z",
     "iopub.status.idle": "2025-09-10T19:56:19.489952Z",
     "shell.execute_reply": "2025-09-10T19:56:19.489106Z",
     "shell.execute_reply.started": "2025-09-10T19:56:07.749485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 744326, 'train': 595460, 'eval': 74433, 'test': 74433}\n",
      "Per-type counts:\n",
      "train: {'original': 306161, 'generated': 289299}\n",
      "eval : {'original': 38271, 'generated': 36162}\n",
      "test : {'original': 38270, 'generated': 36163}\n",
      "Per label counts:\n",
      "train: {2: 204637, 1: 201242, 0: 189581}\n",
      "eval : {2: 25676, 1: 25118, 0: 23639}\n",
      "test : {2: 25575, 1: 25038, 0: 23820}\n",
      "Saved splits to data\\debiased\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load debiased MNLI JSONL\n",
    "path = Path('./data/debiased/mnli_aug-z.jsonl')\n",
    "data = pd.read_json(path, lines=True)\n",
    "\n",
    "# Default split ratios and seed\n",
    "train_frac, eval_frac, test_frac = 0.8, 0.1, 0.1\n",
    "seed = 42\n",
    "\n",
    "# Two-step split with stratification by 'type' to preserve distribution across splits\n",
    "# 1) Split into train and temp (eval+test)\n",
    "temp_frac = eval_frac + test_frac\n",
    "stratify_train = data['type'] if 'type' in data.columns else None\n",
    "train_df, temp_df = train_test_split(\n",
    "    data,\n",
    "    test_size=temp_frac,\n",
    "    random_state=seed,\n",
    "    shuffle=True,\n",
    "    stratify=stratify_train\n",
    ")\n",
    "\n",
    "# 2) Split temp into eval and test, preserving overall ratios\n",
    "stratify_temp = temp_df['type'] if 'type' in temp_df.columns else None\n",
    "eval_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=(test_frac / temp_frac) if temp_frac > 0 else 0.5,\n",
    "    random_state=seed + 1,\n",
    "    shuffle=True,\n",
    "    stratify=stratify_temp\n",
    ")\n",
    "\n",
    "# Report sizes and per-type counts for sanity\n",
    "print({\n",
    "    'total': len(data),\n",
    "    'train': len(train_df),\n",
    "    'eval': len(eval_df),\n",
    "    'test': len(test_df)\n",
    "})\n",
    "if 'type' in data.columns:\n",
    "    print('Per-type counts:')\n",
    "    print('train:', train_df['type'].value_counts().to_dict())\n",
    "    print('eval :', eval_df['type'].value_counts().to_dict())\n",
    "    print('test :', test_df['type'].value_counts().to_dict())\n",
    "#Print per label counts\n",
    "print('Per label counts:')\n",
    "print('train:', train_df['label'].value_counts().to_dict())\n",
    "print('eval :', eval_df['label'].value_counts().to_dict())\n",
    "print('test :', test_df['label'].value_counts().to_dict())\n",
    "\n",
    "# Optionally save splits (JSONL)\n",
    "out_dir = Path('./data/debiased')\n",
    "train_df.to_json(out_dir / 'mnli_aug-z.train.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "eval_df.to_json(out_dir / 'mnli_aug-z.eval.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "test_df.to_json(out_dir / 'mnli_aug-z.test.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "print('Saved splits to', str(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-10T19:58:18.496699Z",
     "iopub.status.busy": "2025-09-10T19:58:18.495888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇█▇█▇</td></tr><tr><td>eval/f1_macro</td><td>▁▇█▇▇▇</td></tr><tr><td>eval/loss</td><td>▂▁▂▆█▂</td></tr><tr><td>eval/precision_macro</td><td>▁▆█▇▇▇</td></tr><tr><td>eval/recall_macro</td><td>▁▇███▇</td></tr><tr><td>eval/runtime</td><td>▁▁▅▅██</td></tr><tr><td>eval/samples_per_second</td><td>██▃▄▁▁</td></tr><tr><td>eval/steps_per_second</td><td>██▃▄▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.88817</td></tr><tr><td>eval/f1_macro</td><td>0.88861</td></tr><tr><td>eval/loss</td><td>0.33112</td></tr><tr><td>eval/precision_macro</td><td>0.88986</td></tr><tr><td>eval/recall_macro</td><td>0.88808</td></tr><tr><td>eval/runtime</td><td>266.9512</td></tr><tr><td>eval/samples_per_second</td><td>278.826</td></tr><tr><td>eval/steps_per_second</td><td>17.43</td></tr><tr><td>total_flos</td><td>1.3269533217921792e+17</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-bird-13</strong> at: <a href='https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased/runs/ent4c7l5' target=\"_blank\">https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased/runs/ent4c7l5</a><br> View project at: <a href='https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased' target=\"_blank\">https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250921_084124-ent4c7l5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Code\\Gemma Finetuning for NLI\\wandb\\run-20250921_201049-obrcqb4y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased/runs/obrcqb4y' target=\"_blank\">decent-sunset-14</a></strong> to <a href='https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased' target=\"_blank\">https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased/runs/obrcqb4y' target=\"_blank\">https://wandb.ai/rahul-krishnan27-universit-t-trier/mnli-debiased/runs/obrcqb4y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 595460/595460 [00:40<00:00, 14608.39 examples/s]\n",
      "Map: 100%|██████████| 74433/74433 [00:05<00:00, 12919.73 examples/s]\n",
      "Map: 100%|██████████| 74433/74433 [00:04<00:00, 15036.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': [\"JERUSALEM (AP) - Israel's top court Wednesday upheld the right of the country's largest daily paper to publish editorially critical of the government, a decision that outraged politicians and human rights groups but won admiration from most Israelis.\", 'And each year at festival time, Edinburgh willingly gives its streets over to stilt walkers, automatons, satirists, and barbershop quartets along with 500,000 visitors belying its reputation for being sober and staid.'], 'hypothesis': ['Politicians and other groups were upset by the ruling.', 'With all the people and activity, Edinburgh is quite chaotic around festival time. '], 'label': [0, 1], 'input_ids': [[101, 6744, 1006, 9706, 1007, 1011, 3956, 1005, 1055, 2327, 2457, 9317, 16813, 1996, 2157, 1997, 1996, 2406, 1005, 1055, 2922, 3679, 3259, 2000, 10172, 8368, 2135, 4187, 1997, 1996, 2231, 1010, 1037, 3247, 2008, 23558, 8801, 1998, 2529, 2916, 2967, 2021, 2180, 17005, 2013, 2087, 28363, 1012, 102, 8801, 1998, 2060, 2967, 2020, 6314, 2011, 1996, 6996, 1012, 102], [101, 1998, 2169, 2095, 2012, 2782, 2051, 1010, 5928, 18110, 3957, 2049, 4534, 2058, 2000, 25931, 2102, 22559, 1010, 8285, 18900, 5644, 1010, 2938, 15735, 12837, 1010, 1998, 13362, 22231, 2361, 8530, 2015, 2247, 2007, 3156, 1010, 2199, 5731, 19337, 14147, 2049, 5891, 2005, 2108, 17358, 1998, 2358, 14326, 1012, 102, 2007, 2035, 1996, 2111, 1998, 4023, 1010, 5928, 2003, 3243, 19633, 2105, 2782, 2051, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\AppData\\Local\\Temp\\ipykernel_23176\\2582902472.py:117: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93045' max='93045' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [93045/93045 12:59:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.334551</td>\n",
       "      <td>0.875821</td>\n",
       "      <td>0.876144</td>\n",
       "      <td>0.876054</td>\n",
       "      <td>0.876097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>0.309098</td>\n",
       "      <td>0.888692</td>\n",
       "      <td>0.889359</td>\n",
       "      <td>0.888806</td>\n",
       "      <td>0.889008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.327444</td>\n",
       "      <td>0.891003</td>\n",
       "      <td>0.892059</td>\n",
       "      <td>0.890956</td>\n",
       "      <td>0.891341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.373436</td>\n",
       "      <td>0.891110</td>\n",
       "      <td>0.891512</td>\n",
       "      <td>0.891265</td>\n",
       "      <td>0.891357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.455744</td>\n",
       "      <td>0.888154</td>\n",
       "      <td>0.888611</td>\n",
       "      <td>0.888076</td>\n",
       "      <td>0.888293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4653' max='4653' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4653/4653 02:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation - MNLI Debiased:\n",
      "{'eval_loss': 0.3783320486545563, 'eval_accuracy': 0.8899547243830022, 'eval_precision_macro': 0.8902772518489513, 'eval_recall_macro': 0.8901263932942888, 'eval_f1_macro': 0.8901749240240694, 'eval_runtime': 159.853, 'eval_samples_per_second': 465.634, 'eval_steps_per_second': 29.108, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 438M/438M [06:18<00:00, 1.16MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed model to: ndsanjana/BERT-Base-MNLI-Debiased\n"
     ]
    }
   ],
   "source": [
    "# train_bert_mnli_kaggle.py\n",
    "# Fine-tune BERT-base on MNLI and push final model to HF Hub repo \"{HF_USERNAME}/BERT-Base-MNLI-Orig\"\n",
    "import os\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoConfig\n",
    ")\n",
    "import evaluate\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "def push_folder_to_hub(folder_path, repo_id, token):\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id=repo_id, token=token, exist_ok=True)\n",
    "    api.upload_folder(folder_path=folder_path, repo_id=repo_id, token=token, repo_type=\"model\")\n",
    "\n",
    "def preprocess_batch(examples, tokenizer, max_length=256):\n",
    "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True, max_length=max_length)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_id\", default=\"bert-base-uncased\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"/kaggle/working/bert_mnli_out\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train_bs\", type=int, default=8)\n",
    "    parser.add_argument(\"--eval_bs\", type=int, default=16)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5)\n",
    "    parser.add_argument(\"--max_len\", type=int, default=256)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    HF_TOKEN = \"\"\n",
    "    HF_USERNAME = \"ndsanjana\"\n",
    "    wandb_token = os.environ.get(\"WANDB_TOKEN\")\n",
    "    # os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"mnli-debiased\"\n",
    "    import wandb\n",
    "    wandb.login(key=wandb_token)  # Usually you only need to run this once per machine\n",
    "    wandb.init(entity=\"rahul-krishnan27-universit-t-trier\", project=\"mnli-debiased\")\n",
    "    if not HF_TOKEN or not HF_USERNAME:\n",
    "        raise RuntimeError(\"Set HF_TOKEN and HF_USERNAME as Kaggle Secrets before running.\")\n",
    "    repo_id = f\"{HF_USERNAME}/BERT-Base-MNLI-Debiased\"\n",
    "\n",
    "    # Login for uploads\n",
    "    login(token=HF_TOKEN)\n",
    "\n",
    "    # Load tokenizer & (sequence-classification) model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id, use_fast=True)\n",
    "    # Create config with correct num_labels and explicit label mapping\n",
    "    cfg = AutoConfig.from_pretrained(args.model_id)\n",
    "    cfg.num_labels = 3\n",
    "    # Explicit class order: 0=ENTAILMENT, 1=NEUTRAL, 2=CONTRADICTION, plus LABEL_* for eval compatibility\n",
    "    cfg.id2label = {0: \"LABEL_0\", 1: \"LABEL_1\", 2: \"LABEL_2\"}\n",
    "    cfg.label2id = {\"LABEL_0\": 0, \"LABEL_1\": 1, \"LABEL_2\": 2,\n",
    "                    \"ENTAILMENT\": 0, \"NEUTRAL\": 1, \"CONTRADICTION\": 2}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_id, config=cfg)\n",
    "    model.config.hidden_dropout_prob = 0.2\n",
    "    model.config.attention_probs_dropout_prob = 0.1\n",
    "    # Load MNLI\n",
    "    train = Dataset.from_pandas(train_df)\n",
    "    eval_set = Dataset.from_pandas(eval_df)\n",
    "    test = Dataset.from_pandas(test_df)\n",
    "    # Preprocess (batched)\n",
    "    train = train.map(lambda x: preprocess_batch(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in train.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "    eval_set = eval_set.map(lambda x: preprocess_batch(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in eval_set.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "    test = test.map(lambda x: preprocess_batch(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in test.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "    print(eval_set[:2])\n",
    "    # Combine matched + mismatched as test set for final evaluation\n",
    "    #test = val_matched.concatenate(val_mismatched)\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # Metrics\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    metric_prec = evaluate.load(\"precision\")\n",
    "    metric_rec = evaluate.load(\"recall\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "            \"precision_macro\": metric_prec.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "            \"recall_macro\": metric_rec.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "            \"f1_macro\": metric_f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "            }\n",
    "\n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"your_output_dir\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=5,\n",
    "        lr_scheduler_type=\"constant_with_warmup\",\n",
    "        warmup_steps=2000,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        logging_steps=200,\n",
    "        fp16=True,  # if you have GPU support\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=eval_set,   # evaluate on combined validation during training\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "    # Final evaluation on test (validation_matched + validation_mismatched)\n",
    "\n",
    "    eval_res = trainer.evaluate(eval_dataset=test)\n",
    "    print(\"Final evaluation - MNLI Debiased:\")\n",
    "    print(eval_res)\n",
    "\n",
    "    # Save locally and push to Hugging Face hub\n",
    "    final_dir = os.path.join(args.output_dir, \"final_model_debiased\")\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    model.save_pretrained(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "    push_folder_to_hub(final_dir, repo_id, HF_TOKEN)\n",
    "    print(\"Pushed model to:\", repo_id)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # kaggle_train_gemma_nli.py\n",
    "# # Kaggle-ready trainer: Gemma-3-270m + LoRA -> merge -> push to HF Hub repo \"{HF_USERNAME}/gemma3_mnli_orig\"\n",
    "# import os\n",
    "# import argparse\n",
    "# from datasets import load_dataset\n",
    "# import evaluate\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoConfig,\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorWithPadding\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# from peft import PeftModel\n",
    "# from huggingface_hub import HfApi, login\n",
    "\n",
    "# def get_model_and_tokenizer(model_name, num_labels, token):\n",
    "#     tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=token)\n",
    "#     try:\n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, trust_remote_code=True, token=token)\n",
    "#     except Exception:\n",
    "#         from transformers import AutoModel, AutoConfig, PreTrainedModel\n",
    "#         import torch.nn as nn, torch\n",
    "#         cfg = AutoConfig.from_pretrained(model_name, token=token)\n",
    "#         cfg.num_labels = num_labels\n",
    "#         base = AutoModel.from_pretrained(model_name, config=cfg, trust_remote_code=True, token=token)\n",
    "#         class SimpleClsModel(PreTrainedModel):\n",
    "#             config_class = type(cfg)\n",
    "#             def __init__(self, base_model, config):\n",
    "#                 super().__init__(config)\n",
    "#                 self.base = base_model\n",
    "#                 hidden = base_model.config.hidden_size\n",
    "#                 self.pooler = nn.Linear(hidden, hidden)\n",
    "#                 self.classifier = nn.Linear(hidden, config.num_labels)\n",
    "#             def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "#                 outputs = self.base(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "#                 last = outputs.last_hidden_state\n",
    "#                 pooled = last.mean(dim=1)\n",
    "#                 pooled = torch.tanh(self.pooler(pooled))\n",
    "#                 logits = self.classifier(pooled)\n",
    "#                 from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "#                 loss = None\n",
    "#                 if labels is not None:\n",
    "#                     import torch.nn.functional as F\n",
    "#                     if self.config.num_labels == 1:\n",
    "#                         loss = F.mse_loss(logits.view(-1), labels.view(-1).float())\n",
    "#                     else:\n",
    "#                         loss = F.cross_entropy(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "#                 return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=None, attentions=None)\n",
    "#         model = SimpleClsModel(base, cfg)\n",
    "#     return tok, model\n",
    "\n",
    "# def preprocess(batch, tokenizer, max_length=256):\n",
    "#     # ensure truncation; do NOT set return_tensors here\n",
    "#     return tokenizer(batch[\"premise\"], batch[\"hypothesis\"], truncation=True, max_length=max_length)\n",
    "\n",
    "# def push_folder_to_hub(folder_path, repo_id, token):\n",
    "#     api = HfApi()\n",
    "#     # create repo if not exists\n",
    "#     api.create_repo(repo_id=repo_id, token=token, exist_ok=True)\n",
    "#     # upload entire folder\n",
    "#     api.upload_folder(folder_path=folder_path, repo_id=repo_id, token=token, repo_type=\"model\")\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--model_name\", default=\"google/gemma-3-270m\")\n",
    "#     parser.add_argument(\"--output_dir\", default=\"/kaggle/working/gemma_nli_out\")\n",
    "#     # fixed sensible defaults (term-paper friendly)\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8)\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=16)\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "#     parser.add_argument(\"--lr\", type=float, default=2e-5)\n",
    "#     parser.add_argument(\"--lora_r\", type=int, default=16)\n",
    "#     parser.add_argument(\"--lora_alpha\", type=int, default=32)\n",
    "#     parser.add_argument(\"--max_len\", type=int, default=256)\n",
    "#     args, unknown = parser.parse_known_args()\n",
    "#     from kaggle_secrets import UserSecretsClient\n",
    "#     user_secrets = UserSecretsClient()\n",
    "#     HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "#     HF_USERNAME = user_secrets.get_secret(\"HF_USERNAME\")\n",
    "\n",
    "#     #HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "#     #HF_USERNAME = os.environ.get(\"HF_USERNAME\")\n",
    "#     if not HF_TOKEN or not HF_USERNAME:\n",
    "#         raise RuntimeError(\"Set HF_TOKEN and HF_USERNAME as Kaggle Secrets or environment variables before running.\")\n",
    "\n",
    "#     repo_id = f\"{HF_USERNAME}/gemma3_mnli_debiased\"\n",
    "#     from huggingface_hub import login\n",
    "#     from datasets import Dataset\n",
    "#     login(token=HF_TOKEN)\n",
    "#     num_labels = 3\n",
    "#     tokenizer, model = get_model_and_tokenizer(args.model_name, num_labels, token=HF_TOKEN)\n",
    "\n",
    "#     # Apply LoRA / PEFT\n",
    "#     lora_config = LoraConfig(\n",
    "#         task_type=\"SEQ_CLS\",\n",
    "#         r=args.lora_r,\n",
    "#         lora_alpha=args.lora_alpha,\n",
    "#         target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#         inference_mode=False\n",
    "#     )\n",
    "#     model = get_peft_model(model, lora_config)\n",
    "#         # Unfreeze typical head names — print names first to confirm an exact substring\n",
    "#     for n,p in model.named_parameters():\n",
    "#         if any(k in n.lower() for k in (\"classifier\",\"pooler\",\"out_proj\",\"dense\")):\n",
    "#             p.requires_grad = True\n",
    "#     # Load MNLI\n",
    "#     #dataset = load_dataset(\"multi_nli\")\n",
    "#     train = Dataset.from_pandas(train_df)\n",
    "#     eval_set = Dataset.from_pandas(eval_df)\n",
    "\n",
    "#     #eval_set = dataset[\"validation_matched\"]\n",
    "#     n = len(train)               # total samples\n",
    "#     print(n)\n",
    "#     # tokenization (batched)\n",
    "#     train = train.map(lambda x: preprocess(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in train.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "#     eval_set = eval_set.map(lambda x: preprocess(x, tokenizer, args.max_len), batched=True, remove_columns=[c for c in eval_set.column_names if c not in (\"premise\",\"hypothesis\",\"label\")])\n",
    "\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer)\n",
    "#     metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "#     def compute_metrics(eval_pred):\n",
    "#         logits, labels = eval_pred\n",
    "#         preds = logits.argmax(axis=-1)\n",
    "#         return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=args.output_dir,\n",
    "#         per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "#         per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
    "#         num_train_epochs=args.epochs,\n",
    "#         learning_rate=args.lr,\n",
    "#         eval_strategy=\"epoch\",   # older/newer transformers differences: this is compatible on many versions; if error, use eval_strategy\n",
    "#         save_strategy=\"epoch\",\n",
    "#         logging_strategy=\"steps\",\n",
    "#         logging_steps=100,\n",
    "#         remove_unused_columns=True,\n",
    "#         fp16=True,\n",
    "#         push_to_hub=False,   # we will push merged model ourselves\n",
    "#         report_to=\"none\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train,\n",
    "#         eval_dataset=eval_set,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "#     trainer.evaluate()\n",
    "\n",
    "#     # Save the PEFT adapter first\n",
    "#     adapter_dir = os.path.join(args.output_dir, \"peft_adapter_debias\")\n",
    "#     model.save_pretrained(adapter_dir)\n",
    "\n",
    "#     # Merge PEFT weights into a standalone model for inference\n",
    "#     # PeftModel.merge_and_unload() returns a merged model (may be framework dependent)\n",
    "#     try:\n",
    "#         # If model is a PeftModel, merge adapters\n",
    "#         if hasattr(model, \"merge_and_unload\"):\n",
    "#             merged_model = model.merge_and_unload()\n",
    "#         else:\n",
    "#             # fallback: load base and PeftModel then merge\n",
    "#             base = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels, trust_remote_code=True, token=HF_TOKEN)\n",
    "#             pft = PeftModel.from_pretrained(base, adapter_dir, token=HF_TOKEN)\n",
    "#             merged_model = pft.merge_and_unload()\n",
    "#     except Exception as e:\n",
    "#         print(\"Warning: merging adapters failed — saving adapter only. Error:\", e)\n",
    "#         merged_model = None\n",
    "\n",
    "#     # Save merged model if available; otherwise save adapter + base reference\n",
    "#     if merged_model is not None:\n",
    "#         merged_dir = os.path.join(args.output_dir, \"merged_model_debias\")\n",
    "#         merged_model.save_pretrained(merged_dir)\n",
    "#         tokenizer.save_pretrained(merged_dir)\n",
    "#         # push merged_dir to Hub\n",
    "#         push_folder_to_hub(merged_dir, repo_id, HF_TOKEN)\n",
    "#         print(\"Merged model pushed to:\", repo_id)\n",
    "#     else:\n",
    "#         # push adapter + instruction for loading\n",
    "#         push_folder_to_hub(adapter_dir, repo_id, HF_TOKEN)\n",
    "#         tokenizer.save_pretrained(adapter_dir)\n",
    "#         print(\"Adapter pushed to:\", repo_id, \" — note: loading requires base model + adapter\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Evaluate fine-tuned model on test_df with accuracy, precision, recall, F1\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from datasets import Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "# from peft import PeftModel\n",
    "# import evaluate\n",
    "\n",
    "# # Ensure the split cell has run\n",
    "# assert 'test_df' in globals(), \"test_df not found. Run the split cell first.\"\n",
    "\n",
    "# # Paths consistent with the training cell's defaults\n",
    "# output_dir = \"/kaggle/working/gemma_nli_out\"\n",
    "# merged_dir = os.path.join(output_dir, \"merged_model_debias\")\n",
    "# adapter_dir = os.path.join(output_dir, \"peft_adapter_debias\")\n",
    "# model_name = \"google/gemma-3-270m\"\n",
    "# num_labels = 3\n",
    "# max_len = 256\n",
    "\n",
    "# # Tokenizer: prefer merged_dir if available, else base model\n",
    "# tok_path = merged_dir if os.path.isdir(merged_dir) else model_name\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tok_path, use_fast=True)\n",
    "\n",
    "# # Model: prefer merged standalone model; else base + adapter\n",
    "# model = None\n",
    "# if os.path.isdir(merged_dir):\n",
    "#     try:\n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(merged_dir, trust_remote_code=True)\n",
    "#     except Exception as e:\n",
    "#         print(\"Warning: failed to load merged model; falling back to base + adapter.\", e)\n",
    "\n",
    "# if model is None:\n",
    "#     base = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, trust_remote_code=True)\n",
    "#     model = PeftModel.from_pretrained(base, adapter_dir)\n",
    "\n",
    "# # Convert pandas -> HF Dataset\n",
    "# test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# # Tokenize\n",
    "# def preprocess_eval(batch):\n",
    "#     return tokenizer(batch[\"premise\"], batch[\"hypothesis\"], truncation=True, max_length=max_len)\n",
    "\n",
    "# keep_cols = [\"premise\", \"hypothesis\", \"label\"]\n",
    "# remove_cols = [c for c in test_ds.column_names if c not in keep_cols]\n",
    "# test_ds_tok = test_ds.map(preprocess_eval, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# eval_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     dataloader_drop_last=False,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=eval_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# pred_out = trainer.predict(test_ds_tok)\n",
    "# logits = pred_out.predictions\n",
    "# labels = pred_out.label_ids\n",
    "# preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "# acc = evaluate.load(\"accuracy\")\n",
    "# prec = evaluate.load(\"precision\")\n",
    "# rec = evaluate.load(\"recall\")\n",
    "# f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# results = {\n",
    "#     \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "#     \"precision\": prec.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "#     \"recall\": rec.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "#     \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "# }\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "    'https://github.com/Aatlantise/syntactic-augmentation-nli/blob/master/datasets/comb_orig_large.tsv',\n",
    "    'https://github.com/Aatlantise/syntactic-augmentation-nli/blob/master/datasets/comb_trsf_large.tsv'\n",
    "    'https://github.com/Aatlantise/syntactic-augmentation-nli/blob/master/datasets/inv_orig_large.tsv',\n",
    "    'https://github.com/Aatlantise/syntactic-augmentation-nli/blob/master/datasets/inv_trsf_large.tsv',\n",
    "    'https://github.com/Aatlantise/syntactic-augmentation-nli/blob/master/datasets/pass_orig_large.tsv',\n",
    "    'https://github.com/Aatlantise/syntactic-augmentation-nli/blob/master/datasets/pass_trsf_large.tsv'\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
